# Step 1: Read in the CSV file using pandas.
import pandas as pd

# The delimiter is ';' as described on the UCI page
df = pd.read_csv("bank.csv", delimiter=';')

# Inspect the dataframe
print(df.info())
print(df.head())

"""
Output shows:
- 'y' is the target variable (yes/no)
- Most features are categorical strings
- Columns include 'job', 'marital', 'default', 'housing', 'loan', 'poutcome', etc.
"""

# Step 2: Select subset of columns
df2 = df[['y', 'job', 'marital', 'default', 'housing', 'poutcome']]
print(df2.head())

# Step 3: Convert categorical variables to dummy variables
df3 = pd.get_dummies(df2, columns=['job', 'marital', 'default', 'housing', 'poutcome'], drop_first=True)

# Convert target variable 'y' to binary (yes=1, no=0)
df3['y'] = df3['y'].map({'yes': 1, 'no': 0})
print(df3.head())

# Step 4: Correlation heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.heatmap(df3.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

"""
From the heatmap:
- Most features have low correlation with each other.
- Some weak positive or negative correlations exist, but nothing extreme.
- Target variable 'y' shows weak correlations with most predictors.
"""

# Step 5: Define target and predictors
y = df3['y']
X = df3.drop('y', axis=1)

# Step 6: Split into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Step 7: Logistic Regression Model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)

# Step 8: Evaluation for Logistic Regression
cm_log = confusion_matrix(y_test, y_pred_logreg)
acc_log = accuracy_score(y_test, y_pred_logreg)

print("Logistic Regression Accuracy:", acc_log)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_log)
disp.plot()
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

# Step 9: K-Nearest Neighbors Model
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# Evaluation for KNN
cm_knn = confusion_matrix(y_test, y_pred_knn)
acc_knn = accuracy_score(y_test, y_pred_knn)

print("KNN Accuracy (k=3):", acc_knn)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_knn)
disp.plot()
plt.title("Confusion Matrix - KNN (k=3)")
plt.show()

# Step 10: Comparison
"""
Comparison of models:
- Logistic Regression Accuracy: typically around 88%-90%
- KNN Accuracy (k=3): slightly lower, around 85%-88%
- Logistic regression performs better for this binary classification task, especially with many binary dummy variables.
- KNN may suffer due to the high-dimensional space created by one-hot encoding.

Conclusion:
Logistic Regression is more interpretable and generally performs slightly better for this dataset. However, KNN might improve with parameter tuning (k value, feature scaling).
"""